# MACHINE-LEARNING-MODEL-IMPLEMENTATION

COMPANY:CODTECH IT SOLUTIONS
NAME:AKSHAY.M
INTERN ID:CT08NEP
DOMAIN:PYTHON
BATCH DURATION:January 15th/2025 to February 15th/2025

The provided Python code demonstrates the development of a machine learning model for detecting spam messages using the Naive Bayes classifier. The process involves data preprocessing, feature extraction, model training, and evaluation. Here's a step-by-step explanation of the code:

### 1. **Library Imports**
The code imports several libraries, including `pandas` and `numpy` for data manipulation, `nltk` for natural language processing, and `scikit-learn` for machine learning tasks. The `stopwords` module from NLTK is used to remove common, insignificant words from the text.

### 2. **Dataset Loading**
A dataset is loaded from a CSV file named `spam.csv`, which contains spam and non-spam (ham) messages. The dataset is filtered to keep only the relevant columns, which are renamed as `label` and `message`. The labels are mapped to binary values: `0` for ham (non-spam) and `1` for spam.

### 3. **Text Preprocessing**
To prepare the textual data for model training, a preprocessing function is defined:
- Converts the text to lowercase to ensure uniformity.
- Removes punctuation using Python's `string.punctuation`.
- Tokenizes the text into individual words.
- Removes stopwords (common words like "the", "and") using the NLTK `stopwords` corpus.
The `preprocess_text` function is applied to the `message` column, transforming it into cleaned and simplified text.

### 4. **Feature Extraction**
The cleaned messages are converted into numerical representations using the **TF-IDF (Term Frequency-Inverse Document Frequency)** technique. This method assigns importance scores to words based on their frequency in individual messages and their rarity across the dataset. The `TfidfVectorizer` is configured to extract features from the top 5000 words, reducing the dimensionality of the dataset.

### 5. **Train-Test Split**
The dataset is split into training (80%) and testing (20%) sets using `train_test_split`. The training set is used to train the Naive Bayes classifier, while the testing set evaluates its performance.

### 6. **Model Training**
A Naive Bayes model (`MultinomialNB`) is used, which is well-suited for text classification tasks. It is trained on the training set (`X_train` and `y_train`) using the `fit` method.

### 7. **Making Predictions**
The trained model predicts labels for the test data (`X_test`) using the `predict` method, producing a set of predicted values (`y_pred`).

### 8. **Model Evaluation**
The model's performance is evaluated using two key metrics:
- **Accuracy:** The proportion of correctly predicted labels out of the total test samples, calculated using `accuracy_score`.
- **Classification Report:** A detailed breakdown of precision, recall, F1-score, and support for both classes (spam and ham), generated by `classification_report`.

### 9. **Output Results**
Finally, the script prints the modelâ€™s accuracy and the classification report to provide insight into its effectiveness in identifying spam messages.

### Summary
This code effectively illustrates how to build a text classification pipeline using machine learning. It covers essential steps such as text preprocessing, feature extraction with TF-IDF, and model training using Naive Bayes. The approach is particularly useful for spam detection and similar text classification problems due to its simplicity, efficiency, and robustness.
